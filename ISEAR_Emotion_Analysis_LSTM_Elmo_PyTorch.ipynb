{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"]=\"tfhub_modules\"\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from allennlp.modules.elmo import batch_to_ids\n",
    "\n",
    "class ISEARDataset(object):\n",
    "  FILENAME = \"data/isear_databank.csv\"\n",
    "  EMOTION_CLASSES = [\"anger\", \"disgust\", \"fear\", \"guilt\", \"joy\", \"sadness\", \"shame\"]\n",
    "  EMOTION_CLASSES_DICT = {\"anger\": 0, \"disgust\": 1, \"fear\": 2, \"guilt\": 3, \"joy\": 4, \"sadness\": 5, \"shame\": 6}\n",
    "  RANDOM_STATE = 41\n",
    "\n",
    "  def get_classes(self):\n",
    "    return self.EMOTION_CLASSES\n",
    "  \n",
    "  def get_classes_dict(self):\n",
    "    return self.EMOTION_CLASSES_DICT\n",
    "\n",
    "  def __init__(self, n_items=0):\n",
    "    data = pd.read_csv(self.FILENAME)\n",
    "    if n_items > 0:\n",
    "      data = data.iloc[0:n_items,:]\n",
    "    data[\"text\"] = data[\"SIT\"]\n",
    "    data[\"emotion\"] = data[\"Field1\"]\n",
    "    for emotion in self.get_classes():\n",
    "      data.loc[data[\"emotion\"] == emotion, \"emotion_int\"] = self.get_classes_dict()[emotion]\n",
    "    self.X = np.array(batch_to_ids(data[\"text\"].values).tolist())\n",
    "    self.y = data[\"emotion_int\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (4292, 889, 50)\n",
      "y_train.shape: (4292)\n",
      "X_valid.shape: (1074, 889, 50)\n",
      "y_valid.shape: (1074)\n",
      "X_test.shape: (2300, 889, 50)\n",
      "y_test.shape: (2300)\n"
     ]
    }
   ],
   "source": [
    "isear_dataset = ISEARDataset()\n",
    "# isear_dataset = ISEARDataset(200)\n",
    "X_train, X_test, y_train, y_test = train_test_split(isear_dataset.X, isear_dataset.y, test_size=0.3, random_state=isear_dataset.RANDOM_STATE, stratify=isear_dataset.y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=isear_dataset.RANDOM_STATE, stratify=y_train)\n",
    "\n",
    "print(\"X_train.shape: (%d, %d, %d)\" % X_train.shape)\n",
    "print(\"y_train.shape: (%d)\" % y_train.shape)\n",
    "\n",
    "print(\"X_valid.shape: (%d, %d, %d)\" % X_valid.shape)\n",
    "print(\"y_valid.shape: (%d)\" % y_valid.shape)\n",
    "\n",
    "print(\"X_test.shape: (%d, %d, %d)\" % X_test.shape)\n",
    "print(\"y_test.shape: (%d)\" % y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class dictionary: {'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n",
      "class labels: ['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']\n",
      "number of bins: 7\n"
     ]
    }
   ],
   "source": [
    "dic = isear_dataset.get_classes_dict()\n",
    "labels = isear_dataset.get_classes()\n",
    "n_classes = len(labels)\n",
    "print(\"class dictionary: %s\" % dic)\n",
    "print(\"class labels: %s\" % labels)\n",
    "print(\"number of bins: %s\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins: [0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = list(range(0, n_classes + 1))\n",
    "print(\"bins:\", bins)\n",
    "hist, _ = np.histogram(y_train, bins=bins)\n",
    "\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "plt.bar(y_pos, hist, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Number')\n",
    "plt.title('Emotions')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape: torch.Size([4292, 1, 1, 1])\n",
      "y_onehot.shape: torch.Size([4292, 7, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch import autograd\n",
    "\n",
    "def make_one_hot(labels, C=2):\n",
    "    '''\n",
    "    Converts an integer label torch.autograd.Variable to a one-hot Variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : torch.autograd.Variable of torch.cuda.LongTensor\n",
    "        N x 1 x H x W, where N is batch size. \n",
    "        Each value is an integer representing correct classification.\n",
    "    C : integer. \n",
    "        number of classes in labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target : torch.autograd.Variable of torch.cuda.FloatTensor\n",
    "        N x C x H x W, where C is class number. One-hot encoded.\n",
    "    '''\n",
    "    one_hot = torch.FloatTensor(labels.size(0), C, labels.size(2), labels.size(3)).zero_()\n",
    "    target = one_hot.scatter_(1, labels.data, 1)\n",
    "    \n",
    "    target = autograd.Variable(target)\n",
    "        \n",
    "    return target\n",
    "  \n",
    "y = torch.LongTensor(y_train).view(-1, 1, 1, 1)\n",
    "print(\"y.shape:\", y.shape)\n",
    "y_onehot = make_one_hot(y, C=7)\n",
    "print(\"y_onehot.shape:\", y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class ISEAR_Tensor_Dataset(data.TensorDataset):\n",
    "  \n",
    "  def __init__(self, text, emotion, num_class=2):\n",
    "    X = torch.Tensor(text.astype('float'))\n",
    "    y = torch.LongTensor(emotion).view(-1, 1, 1, 1)\n",
    "    y_onehot = make_one_hot(y, num_class)\n",
    "    y_onehot = y_onehot.view(y_onehot.shape[0], y_onehot.shape[1])\n",
    "    tensors = []\n",
    "    tensors.append(X)\n",
    "    tensors.append(y_onehot)\n",
    "    super().__init__(*tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ISEAR_Tensor_Dataset(X_train, y_train, num_class=7)\n",
    "valid_dataset = ISEAR_Tensor_Dataset(X_valid, y_valid, num_class=7)\n",
    "test_dataset = ISEAR_Tensor_Dataset(X_test, y_test, num_class=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.tensors[0].shape: torch.Size([4292, 889, 50])\n",
      "train_dataset.tensors[1].shape: torch.Size([4292, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset.tensors[0].shape:\", train_dataset.tensors[0].shape)\n",
    "print(\"train_dataset.tensors[1].shape:\", train_dataset.tensors[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length: 4292\n",
      "valid_dataset length: 1074\n",
      "test_dataset length: 2300\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset length:\", len(train_dataset))\n",
    "print(\"valid_dataset length:\", len(valid_dataset))\n",
    "print(\"test_dataset length:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Elmo Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/allennlp/commands/find_learning_rate.py:54: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2823, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/events.py\", line 88, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\", line 164, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/pylabtools.py\", line 314, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1422, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  import matplotlib; matplotlib.use('Agg') # pylint: disable=multiple-statements,wrong-import-position\n",
      "01/18/2019 11:43:46 - INFO - allennlp.commands.elmo -   Initializing ELMo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15912526845932007"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"\n",
    "\n",
    "elmo_embedder = ElmoEmbedder(options_file, weight_file)\n",
    "tokens = [\"I\", \"ate\", \"an\", \"apple\", \"for\", \"breakfast\"]\n",
    "vectors = elmo_embedder.embed_sentence(tokens)\n",
    "\n",
    "import scipy\n",
    "vectors2 = elmo_embedder.embed_sentence([\"I\", \"ate\", \"a\", \"carrot\", \"for\", \"breakfast\"])\n",
    "scipy.spatial.distance.cosine(vectors[2][3], vectors2[2][3]) # cosine distance between \"apple\" and \"carrot\" in the last layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors.shape: (3, 6, 256)\n",
      "vectors2.shape: (3, 6, 256)\n",
      "vectors[2][3].shape: (256,)\n",
      "vectors2[2][3].shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "print(\"vectors.shape:\", vectors.shape)\n",
    "print(\"vectors2.shape:\", vectors2.shape)\n",
    "\n",
    "print(\"vectors[2][3].shape:\", vectors[2][3].shape)\n",
    "print(\"vectors2[2][3].shape:\", vectors2[2][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 11:44:00 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, num_output_representations=1, dropout=0)\n",
    "elmo = elmo.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Elmo Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_X_train.shape: torch.Size([1, 889, 50])\n",
      "embedded shape: torch.Size([1, 889, 256])\n"
     ]
    }
   ],
   "source": [
    "shape = train_dataset[0][0].shape\n",
    "experiment_X_train = train_dataset[0][0].view(1, shape[0], shape[1]).to(device)\n",
    "print(\"experiment_X_train.shape:\", experiment_X_train.shape)\n",
    "\n",
    "embedded = elmo(experiment_X_train)\n",
    "print(\"embedded shape:\", embedded['elmo_representations'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3055, -0.5664, -0.0241,  ...,  0.0963,  0.1497,  0.0465],\n",
       "        [ 0.0755,  0.2156, -0.3852,  ...,  0.0491, -0.1486, -0.0779],\n",
       "        [ 0.3119,  0.9745, -0.3892,  ...,  0.4795,  0.2638,  0.4102],\n",
       "        [-0.4099,  0.2940, -0.3578,  ...,  0.2550,  0.6134,  0.2625],\n",
       "        [ 0.1248,  0.1070, -0.2953,  ..., -0.2505, -0.0047,  0.2117],\n",
       "        [-0.4632,  0.5689,  0.2057,  ..., -0.4176, -0.3930, -0.1277]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "sentences = [\n",
    "  [\"I\", \"ate\", \"an\", \"apple\", \"for\", \"breakfast\"], \n",
    "  [\"I\", \"ate\", \"a\", \"carrot\", \"for\", \"breakfast\"]\n",
    "]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids.to(device))\n",
    "embeddings['elmo_representations'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22335273027420044"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 = embeddings['elmo_representations'][0][0].cpu().detach().numpy()\n",
    "vector2 = embeddings['elmo_representations'][0][1].cpu().detach().numpy()\n",
    "scipy.spatial.distance.cosine(vector1[3], vector2[3]) # cosine distance between \"apple\" and \"carrot\" in the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear_hidden_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_hidden_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.linear_threshold = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_threshold_output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.type())\n",
    "        tmp = F.dropout(F.relu(self.linear_input(x)), self.dropout)\n",
    "        out = F.dropout(F.relu(self.linear_hidden_1(tmp)), self.dropout)\n",
    "        out = F.dropout(F.relu(self.linear_hidden_2(out)), self.dropout)\n",
    "        out = F.sigmoid(self.linear_output(tmp))\n",
    "\n",
    "        tmp = F.dropout(F.relu(self.linear_threshold(tmp)), self.dropout)\n",
    "        threshold = F.sigmoid(self.linear_threshold_output(tmp))\n",
    "\n",
    "        return out, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "  \n",
    "    def _transpose_embedding(self, x):\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "    def __init__(self, elmo_embedding, input_dim, embedding_dim, \n",
    "                 hidden_dim, batch_size, output_dim=1, num_layers=1, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.elmo = elmo_embedding\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, self.num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        embeds = self.elmo(sentence)['elmo_representations'][0]\n",
    "        # print(\"embeds.shape:\", embeds.shape)\n",
    "        transposed_embeds = self._transpose_embedding(embeds)\n",
    "        # print(\"transposed_embeds.shape:\", transposed_embeds.shape)\n",
    "        lstm_out, self.hidden = self.lstm(transposed_embeds)\n",
    "        # print(\"lstm_out.shape:\", lstm_out.shape)\n",
    "        flatten = F.dropout(F.relu(self.linear(lstm_out[-1].view(-1, self.hidden_dim))), self.dropout)\n",
    "        # print(\"flatten.shape:\", flatten.shape)\n",
    "        output = self.sigmoid(flatten)\n",
    "        # print(\"output.shape:\", output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4292, 889, 50])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tensors[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_dataset.tensors[0].shape[1]\n",
    "hidden_dim = 500\n",
    "output_dim = train_dataset.tensors[1].shape[1]\n",
    "dropout = 0.5\n",
    "batch_size = 10\n",
    "model = DNNModel(input_dim, hidden_dim, output_dim, dropout=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "batch_size = 50\n",
    "output_dim = n_classes\n",
    "model = LSTMClassifier(elmo, input_dim, embedding_dim, hidden_dim, batch_size, output_dim)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': <__main__.ISEAR_Tensor_Dataset at 0x7f00e0bf8f28>,\n",
       " 'batch_size': 50,\n",
       " 'num_workers': 0,\n",
       " 'collate_fn': <function torch.utils.data.dataloader.default_collate(batch)>,\n",
       " 'pin_memory': False,\n",
       " 'drop_last': False,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7eff16048320>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7eff16048358>,\n",
       " '_DataLoader__initialized': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, device, optimiser, loss_fn=nn.BCELoss(), log_interval=100):\n",
    "    #####################\n",
    "    # Train model\n",
    "    #####################\n",
    "\n",
    "    # switch model to training mode, clear gradient accumulators\n",
    "    model.train()\n",
    "    # model.hidden = model.init_hidden()\n",
    "    \n",
    "    train_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    all_pred = []\n",
    "    all_target = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        train_loss += loss.data # sum up batch loss\n",
    "        \n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        pred = pred.view(pred.size(0))\n",
    "\n",
    "        target = target.max(1, keepdim=True)[1]\n",
    "        target = target.view(target.size(0))\n",
    "\n",
    "        correct = pred.eq(target.view_as(pred)).sum()\n",
    "        total_correct += correct\n",
    "\n",
    "        all_pred += pred.cpu().numpy().tolist()\n",
    "        all_target += target.cpu().numpy().tolist()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}\\t/\\t{}\\t({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "    \n",
    "    print('Train: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, total_correct, len(data_loader.dataset),\n",
    "        100. * total_correct / len(data_loader.dataset)))\n",
    "    \n",
    "    return train_loss, all_pred, all_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, loss_fn=nn.BCELoss()):\n",
    "    #####################\n",
    "    # Evaluation model\n",
    "    #####################\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    all_pred = []\n",
    "    all_target = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            eval_loss += loss_fn(output, target).data # sum up batch loss\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            pred = pred.view(pred.size(0))\n",
    "            \n",
    "            target = target.max(1, keepdim=True)[1]\n",
    "            target = target.view(target.size(0))\n",
    "            \n",
    "            correct = pred.eq(target.view_as(pred)).sum()\n",
    "            total_correct += correct\n",
    "\n",
    "            all_pred += pred.cpu().numpy().tolist()\n",
    "            all_target += target.cpu().numpy().tolist()\n",
    "\n",
    "    print('Evaluate: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        eval_loss, total_correct, len(data_loader.dataset),\n",
    "        100. * total_correct / len(data_loader.dataset)))\n",
    "    \n",
    "    return eval_loss, all_pred, all_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [44450 x 50], m2: [889 x 889] at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/THC/generic/THCTensorMathBlas.cu:266",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-27b238d65cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b5a0f535f95b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, data_loader, device, optimiser, loss_fn, log_interval)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-b40a61577d06>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_hidden_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_hidden_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [44450 x 50], m2: [889 x 889] at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/THC/generic/THCTensorMathBlas.cu:266"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "log_interval = 10\n",
    "max_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_hist = np.zeros(max_epochs)\n",
    "eval_loss_hist = np.zeros(max_epochs)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "    train_loss, train_pred, train_target = train(epoch, model, train_loader, device, optimiser, loss_fn, log_interval)\n",
    "    train_loss_hist[epoch] = train_loss\n",
    "    \n",
    "    valid_loss, valid_pred, valid_target = evaluate(model, valid_loader, device, loss_fn=nn.BCELoss())\n",
    "    eval_loss_hist[epoch] = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_hist, color='skyblue')\n",
    "plt.plot(eval_loss_hist, color='red')\n",
    "plt.ylabel('Historic Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
