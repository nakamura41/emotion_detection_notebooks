{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"]=\"tfhub_modules\"\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISEARDataset(object):\n",
    "  FILENAME = \"data/isear_databank.csv\"\n",
    "  EMOTION_CLASSES = [\"anger\", \"disgust\", \"fear\", \"guilt\", \"joy\", \"sadness\", \"shame\"]\n",
    "  EMOTION_CLASSES_DICT = {\"anger\": 0, \"disgust\": 1, \"fear\": 2, \"guilt\": 3, \"joy\": 4, \"sadness\": 5, \"shame\": 6}\n",
    "  RANDOM_STATE = 41\n",
    "\n",
    "  def get_classes(self):\n",
    "    return self.EMOTION_CLASSES\n",
    "  \n",
    "  def get_classes_dict(self):\n",
    "    return self.EMOTION_CLASSES_DICT\n",
    "  \n",
    "  def load_data(self, n_items=None):\n",
    "    data = pd.read_csv(self.FILENAME)\n",
    "    if n_items != None:\n",
    "      data = data.iloc[0:n_items,:]\n",
    "    data[\"emotion\"] = data[\"Field1\"]\n",
    "    data[\"text\"] = data[\"SIT\"]\n",
    "    return data[[\"text\", \"emotion\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (4292, 2)\n",
      "valid_data.shape: (1074, 2)\n",
      "test_data.shape: (2300, 2)\n"
     ]
    }
   ],
   "source": [
    "isear_dataset = ISEARDataset()\n",
    "isear_data = isear_dataset.load_data()\n",
    "# isear_data = isear_dataset.load_data(200)\n",
    "train_data, test_data = train_test_split(isear_data, test_size=0.3, random_state=isear_dataset.RANDOM_STATE, stratify=isear_data[\"emotion\"].values)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.2, random_state=isear_dataset.RANDOM_STATE, stratify=train_data.emotion)\n",
    "\n",
    "print(\"train_data.shape: (%d, %d)\" % train_data.shape)\n",
    "print(\"valid_data.shape: (%d, %d)\" % valid_data.shape)\n",
    "print(\"test_data.shape: (%d, %d)\" % test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class dictionary: {'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n",
      "class labels: ['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']\n",
      "number of bins: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins: [0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic = isear_dataset.get_classes_dict()\n",
    "labels = isear_dataset.get_classes()\n",
    "n_classes = len(labels)\n",
    "print(\"class dictionary: %s\" % dic)\n",
    "print(\"class labels: %s\" % labels)\n",
    "print(\"number of bins: %s\" % n_classes)\n",
    "\n",
    "for emotion in labels:\n",
    "  train_data.loc[train_data.emotion == emotion, \"emotion_int\"] = dic[emotion]\n",
    "  valid_data.loc[valid_data.emotion == emotion, \"emotion_int\"] = dic[emotion]\n",
    "  test_data.loc[test_data.emotion == emotion, \"emotion_int\"] = dic[emotion]\n",
    "\n",
    "bins = list(range(0, n_classes + 1))\n",
    "print(\"bins:\", bins)\n",
    "hist, _ = np.histogram(train_data[\"emotion_int\"], bins=bins)\n",
    "\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "plt.bar(y_pos, hist, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Number')\n",
    "plt.title('Emotions')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape: torch.Size([7666, 1, 1, 1])\n",
      "y_onehot.shape: torch.Size([7666, 7, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch import autograd\n",
    "\n",
    "def make_one_hot(labels, C=2):\n",
    "    '''\n",
    "    Converts an integer label torch.autograd.Variable to a one-hot Variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : torch.autograd.Variable of torch.cuda.LongTensor\n",
    "        N x 1 x H x W, where N is batch size. \n",
    "        Each value is an integer representing correct classification.\n",
    "    C : integer. \n",
    "        number of classes in labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target : torch.autograd.Variable of torch.cuda.FloatTensor\n",
    "        N x C x H x W, where C is class number. One-hot encoded.\n",
    "    '''\n",
    "    one_hot = torch.FloatTensor(labels.size(0), C, labels.size(2), labels.size(3)).zero_()\n",
    "    target = one_hot.scatter_(1, labels.data, 1)\n",
    "    \n",
    "    target = autograd.Variable(target)\n",
    "        \n",
    "    return target\n",
    "  \n",
    "y = torch.LongTensor(isear_data['emotion'].apply(lambda x:dic[x])).view(-1, 1, 1, 1)\n",
    "print(\"y.shape:\", y.shape)\n",
    "y_onehot = make_one_hot(y, C=7)\n",
    "print(\"y_onehot.shape:\", y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from allennlp.modules.elmo import batch_to_ids\n",
    "\n",
    "\n",
    "class ISEAR_Tensor_Dataset(data.TensorDataset):\n",
    "  \n",
    "  def __init__(self, input_data, num_class=2):\n",
    "    X = torch.tensor(batch_to_ids(input_data['text'].values))\n",
    "    y = torch.LongTensor(input_data['emotion'].apply(lambda x:dic[x])).view(-1, 1, 1, 1)\n",
    "    y_onehot = make_one_hot(y, num_class)\n",
    "    y_onehot = y_onehot.view(y_onehot.shape[0], y_onehot.shape[1])\n",
    "    tensors = []\n",
    "    tensors.append(X)\n",
    "    tensors.append(y_onehot)\n",
    "    super().__init__(*tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_tensor_dataset = ISEAR_Tensor_Dataset(isear_data, num_class=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"isear_tensor_dataset.tensors[0].shape:\", isear_tensor_dataset.tensors[0].shape)\n",
    "print(\"isear_tensor_dataset.tensors[1].shape:\", isear_tensor_dataset.tensors[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(0.7*len(isear_tensor_dataset))\n",
    "test_length = len(isear_tensor_dataset) - train_length\n",
    "train_dataset, test_dataset = data.random_split(isear_tensor_dataset, (train_length, test_length))\n",
    "\n",
    "train_length = int(0.8*len(train_dataset))\n",
    "valid_length = len(train_dataset) - train_length\n",
    "train_dataset, valid_dataset = data.random_split(train_dataset, (train_length, valid_length))\n",
    "\n",
    "print(\"train_dataset length:\", len(train_dataset))\n",
    "print(\"valid_dataset length:\", len(valid_dataset))\n",
    "print(\"test_dataset length:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, num_output_representations=1, dropout=0)\n",
    "elmo = elmo.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Elmo Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"\n",
    "\n",
    "elmo_embedder = ElmoEmbedder(options_file, weight_file)\n",
    "tokens = [\"I\", \"ate\", \"an\", \"apple\", \"for\", \"breakfast\"]\n",
    "vectors = elmo_embedder.embed_sentence(tokens)\n",
    "\n",
    "import scipy\n",
    "vectors2 = elmo_embedder.embed_sentence([\"I\", \"ate\", \"a\", \"carrot\", \"for\", \"breakfast\"])\n",
    "scipy.spatial.distance.cosine(vectors[2][3], vectors2[2][3]) # cosine distance between \"apple\" and \"carrot\" in the last layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"vectors.shape:\", vectors.shape)\n",
    "print(\"vectors2.shape:\", vectors2.shape)\n",
    "\n",
    "print(\"vectors[2][3].shape:\", vectors[2][3].shape)\n",
    "print(\"vectors2[2][3].shape:\", vectors2[2][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Elmo Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = train_dataset[0][0].shape\n",
    "X_train = train_dataset[0][0].view(1, shape[0], shape[1]).to(device)\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "\n",
    "embedded = elmo(X_train)\n",
    "print(\"embedded shape:\", embedded['elmo_representations'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "sentences = [\n",
    "  [\"I\", \"ate\", \"an\", \"apple\", \"for\", \"breakfast\"], \n",
    "  [\"I\", \"ate\", \"a\", \"carrot\", \"for\", \"breakfast\"]\n",
    "]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids.to(device))\n",
    "embeddings['elmo_representations'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = embeddings['elmo_representations'][0][0].cpu().detach().numpy()\n",
    "vector2 = embeddings['elmo_representations'][0][1].cpu().detach().numpy()\n",
    "scipy.spatial.distance.cosine(vector1[3], vector2[3]) # cosine distance between \"apple\" and \"carrot\" in the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "  \n",
    "    def _transpose_embedding(self, x):\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "    def __init__(self, elmo_embedding, input_dim, embedding_dim, \n",
    "                 hidden_dim, batch_size, output_dim=1, num_layers=1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.elmo = elmo_embedding\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, self.num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        embeds = self.elmo(sentence)['elmo_representations'][0]\n",
    "        # print(\"embeds.shape:\", embeds.shape)\n",
    "        transposed_embeds = self._transpose_embedding(embeds)\n",
    "        # print(\"transposed_embeds.shape:\", transposed_embeds.shape)\n",
    "        lstm_out, self.hidden = self.lstm(transposed_embeds)\n",
    "        # print(\"lstm_out.shape:\", lstm_out.shape)\n",
    "        flatten = self.linear(lstm_out[-1].view(-1, self.hidden_dim))\n",
    "        # print(\"flatten.shape:\", flatten.shape)\n",
    "        output = self.sigmoid(flatten)\n",
    "        # print(\"output.shape:\", output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "batch_size = 10\n",
    "output_dim = n_classes\n",
    "model = LSTMClassifier(elmo, input_dim, embedding_dim, hidden_dim, batch_size, output_dim)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "log_interval = 10\n",
    "max_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "hist = np.zeros(max_epochs)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "    #####################\n",
    "    # Train model\n",
    "    #####################\n",
    "\n",
    "    # switch model to training mode, clear gradient accumulators\n",
    "    model.train()\n",
    "    model.hidden = model.init_hidden()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}\\t/\\t{}\\t({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    hist[epoch] = loss.item()\n",
    "    \n",
    "    #####################\n",
    "    # Evaluation model\n",
    "    #####################\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1][0] # get the index of the max log-probability\n",
    "            target = target.max(1, keepdim=True)[1][0]\n",
    "            correct += pred.eq(target.view_as(pred)).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
