{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"]=\"tfhub_modules\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "print(tf.__version__)\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "matplotlib.pyplot - DEBUG - Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils import to_categorical\n",
    "from keras.engine import Layer\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "RANDOM_SEED = 20190101\n",
    "\n",
    "def set_random(random_seed):\n",
    "    seed(random_seed)\n",
    "    set_random_seed(random_seed)\n",
    "\n",
    "set_random(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISEARDataset(object):\n",
    "  FILENAME = \"data/isear_databank.csv\"\n",
    "  EMOTION_CLASSES = [\"anger\", \"disgust\", \"fear\", \"guilt\", \"joy\", \"sadness\", \"shame\"]\n",
    "  EMOTION_CLASSES_DICT = {\"anger\": 0, \"disgust\": 1, \"fear\": 2, \"guilt\": 3, \"joy\": 4, \"sadness\": 5, \"shame\": 6}\n",
    "  RANDOM_STATE = 41\n",
    "  \n",
    "  def get_classes(self):\n",
    "    return self.EMOTION_CLASSES\n",
    "  \n",
    "  def get_classes_dict(self):\n",
    "    return self.EMOTION_CLASSES_DICT\n",
    "  \n",
    "  def __load_data_file(self):\n",
    "    data = pd.read_csv(self.FILENAME)\n",
    "    data[\"emotion\"] = data[\"Field1\"]\n",
    "    data[\"text\"] = data[\"SIT\"]\n",
    "    return data[[\"text\", \"emotion\"]]\n",
    "\n",
    "  def load_data(self):\n",
    "    train_data = None\n",
    "    test_data = None\n",
    "    \n",
    "    data = self.__load_data_file()\n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.3, random_state=self.RANDOM_STATE, stratify=data[\"emotion\"].values)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - DEBUG - train_data.shape: (4829, 2)\n",
      "root - DEBUG - valid_data.shape: (537, 2)\n",
      "root - DEBUG - test_data.shape: (2300, 2)\n"
     ]
    }
   ],
   "source": [
    "isear_dataset = ISEARDataset()\n",
    "train_data, test_data = isear_dataset.load_data()\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.1, random_state=200, stratify=train_data.emotion)\n",
    "\n",
    "logging.debug(\"train_data.shape: (%d, %d)\" % train_data.shape)\n",
    "logging.debug(\"valid_data.shape: (%d, %d)\" % valid_data.shape)\n",
    "logging.debug(\"test_data.shape: (%d, %d)\" % test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - DEBUG - class dictionary: {'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n",
      "root - DEBUG - class labels: ['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']\n",
      "root - DEBUG - number of bins: 7\n",
      "matplotlib.axes._base - DEBUG - update_title_pos\n",
      "matplotlib.font_manager - DEBUG - findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.\n",
      "matplotlib.font_manager - DEBUG - findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/home/david/anaconda3/envs/tf36/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.\n",
      "matplotlib.axes._base - DEBUG - update_title_pos\n",
      "matplotlib.axes._base - DEBUG - update_title_pos\n",
      "matplotlib.axes._base - DEBUG - update_title_pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins: [0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic = isear_dataset.get_classes_dict()\n",
    "labels = isear_dataset.get_classes()\n",
    "n_classes = len(labels)\n",
    "logging.debug(\"class dictionary: %s\" % dic)\n",
    "logging.debug(\"class labels: %s\" % labels)\n",
    "logging.debug(\"number of bins: %s\" % n_classes)\n",
    "\n",
    "for emotion in labels:\n",
    "  train_data.loc[train_data.emotion == emotion, \"emotion_int\"] = dic[emotion]\n",
    "  valid_data.loc[valid_data.emotion == emotion, \"emotion_int\"] = dic[emotion]\n",
    "  test_data.loc[test_data.emotion == emotion, \"emotion_int\"] = dic[emotion]\n",
    "\n",
    "bins = list(range(0, n_classes + 1))\n",
    "print(\"bins:\", bins)\n",
    "hist, _ = np.histogram(train_data[\"emotion_int\"], bins=bins)\n",
    "\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "plt.bar(y_pos, hist, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Number')\n",
    "plt.title('Emotions')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7194 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=20000\n",
    "texts = train_data.text\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "sequences_valid = tokenizer.texts_to_sequences(valid_data.text)\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data.text)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - DEBUG - Shape of X train, validation and test tensor: (4829, 192), (537, 192), (2300, 192)\n",
      "root - DEBUG - Shape of label train, validation and test tensor: (4829, 7), (537, 7), (2300, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid, maxlen=X_train.shape[1])\n",
    "X_test = pad_sequences(sequences_test, maxlen=X_train.shape[1])\n",
    "\n",
    "y_train = to_categorical(np.asarray(train_data.emotion.apply(lambda x:dic[x])))\n",
    "y_val = to_categorical(np.asarray(valid_data.emotion.apply(lambda x:dic[x])))\n",
    "y_test = to_categorical(np.asarray(test_data.emotion.apply(lambda x:dic[x])))\n",
    "\n",
    "logging.debug('Shape of X train, validation and test tensor: %s, %s, %s' % (X_train.shape, X_val.shape, X_test.shape))\n",
    "logging.debug('Shape of label train, validation and test tensor: %s, %s, %s' % (y_train.shape, y_val.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)\n",
    "\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.sequence_length = 192\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = self.elmo(\n",
    "            inputs={\n",
    "              \"tokens\": K.cast(x, tf.string),\n",
    "              \"sequence_len\": [192]\n",
    "            },\n",
    "            as_dict=True,\n",
    "            signature='tokens',\n",
    "        )['default']\n",
    "        return output\n",
    "\n",
    "    # def compute_mask(self, inputs, mask=None):\n",
    "    #     return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.sequence_length, self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "# Function to build model\n",
    "def build_model(): \n",
    "  sequence_length = X_train.shape[1]\n",
    "  filter_sizes = [3,4,5]\n",
    "  num_filters = 100\n",
    "  drop_rate = 0.5\n",
    "  n_class = 7\n",
    "  EMBEDDING_DIM = 1024\n",
    "  \n",
    "  inputs = layers.Input(shape=(sequence_length,), dtype=\"string\")\n",
    "  embedding = ElmoEmbeddingLayer()(inputs)\n",
    "  reshape = Reshape((sequence_length, EMBEDDING_DIM, 1), name='reshape_1')(embedding)\n",
    "\n",
    "  conv_1 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM), activation='relu',kernel_regularizer=regularizers.l2(0.01), name='conv_1')(reshape)\n",
    "  conv_2 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM), activation='relu',kernel_regularizer=regularizers.l2(0.01), name='conv_2')(reshape)\n",
    "  conv_3 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM), activation='relu',kernel_regularizer=regularizers.l2(0.01), name='conv_3')(reshape)\n",
    "\n",
    "  maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=1, name='maxpool_1')(conv_1)\n",
    "  maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=1, name='maxpool_2')(conv_2)\n",
    "  maxpool_3 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=1, name='maxpool_3')(conv_3)\n",
    "\n",
    "  merged_tensor = concatenate([maxpool_1, maxpool_2, maxpool_3], axis=1, name='concatenate_1')\n",
    "  flatten = Flatten()(merged_tensor)\n",
    "  dropout = Dropout(drop_rate, name='dropout_1')(flatten)\n",
    "  output = Dense(units=n_class, activation='softmax',kernel_regularizer=regularizers.l2(0.01), name='dense_1')(dropout)\n",
    "\n",
    "  # this creates a model that includes\n",
    "  model = Model(inputs, output)\n",
    "\n",
    "  model.summary()\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow - INFO - Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_40 (InputLayer)           (None, 192)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "elmo_embedding_layer_40 (ElmoEm (None, 192, 1024)    4           input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 192, 1024, 1) 0           elmo_embedding_layer_40[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 190, 1, 100)  307300      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 189, 1, 100)  409700      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 188, 1, 100)  512100      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_1 (MaxPooling2D)        (None, 1, 1, 100)    0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_2 (MaxPooling2D)        (None, 1, 1, 100)    0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_3 (MaxPooling2D)        (None, 1, 1, 100)    0           conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 100)    0           maxpool_1[0][0]                  \n",
      "                                                                 maxpool_2[0][0]                  \n",
      "                                                                 maxpool_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 300)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 7)            2107        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,231,211\n",
      "Trainable params: 1,231,211\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from keras.optimizers import Adam, Adadelta, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Build and fit\n",
    "model = build_model()\n",
    "\n",
    "adam = Adam(lr=1e-3, decay=0.0)\n",
    "adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "filepath=\"tmp/weights-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callbacks = [early_stopping, model_checkpoint]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4829 samples, validate on 537 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Unable to get element as bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, int found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function TF_SessionRunCallable> returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-d9c927a64f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     callbacks=callbacks)  # starts training\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Unable to get element as bytes."
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=callbacks)  # starts training\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "def days_hours_minutes_seconds(td):\n",
    "  return td.days, td.seconds//3600, (td.seconds//60)%60, (td.seconds%60)\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "logging.debug(\"%d days, %d hours, %d minutes, %d seconds elapsed\" % (days_hours_minutes_seconds(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('isear_dense_elmo_model.h5')\n",
    "model.save_weights('isear_dense_elmo_weights.h5')\n",
    "\n",
    "logging.debug(\"model and weights is successfully saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_original = [labels[val] for val in np.argmax(y_pred, axis=1).squeeze()]\n",
    "y_test_original = np.asarray(test_data.emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (4,3), fontsize=15):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test_original, y_pred_original, labels=labels)\n",
    "\n",
    "df_cm = pd.DataFrame(\n",
    "    cf_matrix, index=labels, columns=labels, \n",
    ")\n",
    "\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_confusion_matrix(cf_matrix, class_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(y_test_original, y_pred_original)\n",
    "print(\"test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance score for each classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_original, y_pred_original)\n",
    "score_dict = {\n",
    "  \"precision\": precision.round(4),\n",
    "  \"recall\": recall.round(4),\n",
    "  \"f1-score\": fscore.round(4),\n",
    "  \"support\": support.round(4)\n",
    "}\n",
    "score_df = pd.DataFrame(score_dict, index=labels)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance score using micro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_original, y_pred_original, average=\"micro\")\n",
    "score_dict = {\n",
    "  \"precision\": precision.round(4),\n",
    "  \"recall\": recall.round(4),\n",
    "  \"f1-score\": fscore.round(4),\n",
    "  \"support\": support\n",
    "}\n",
    "score_df = pd.DataFrame(score_dict, index=[\"score\"])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance score using macro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_original, y_pred_original, average=\"macro\")\n",
    "score_dict = {\n",
    "  \"precision\": precision.round(4),\n",
    "  \"recall\": recall.round(4),\n",
    "  \"f1-score\": fscore.round(4),\n",
    "  \"support\": support\n",
    "}\n",
    "score_df = pd.DataFrame(score_dict, index=[\"score\"])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance score using weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_original, y_pred_original, average=\"weighted\")\n",
    "score_dict = {\n",
    "  \"precision\": precision.round(4),\n",
    "  \"recall\": recall.round(4),\n",
    "  \"f1-score\": fscore.round(4),\n",
    "  \"support\": support\n",
    "}\n",
    "score_df = pd.DataFrame(score_dict, index=[\"score\"])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen Kappa Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kappa_score = cohen_kappa_score(y_test_original, y_pred_original, labels=labels)\n",
    "print(\"kappa:\", kappa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
