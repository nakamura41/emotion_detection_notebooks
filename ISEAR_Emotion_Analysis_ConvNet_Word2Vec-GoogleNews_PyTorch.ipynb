{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"]=\"tfhub_modules\"\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 20000\n",
    "\n",
    "class ISEARDataset(object):\n",
    "    FILENAME = \"data/isear_databank.csv\"\n",
    "    EMOTION_CLASSES = [\"anger\", \"disgust\", \"fear\", \"guilt\", \"joy\", \"sadness\", \"shame\"]\n",
    "    EMOTION_CLASSES_DICT = {\"anger\": 0, \"disgust\": 1, \"fear\": 2, \"guilt\": 3, \"joy\": 4, \"sadness\": 5, \"shame\": 6}\n",
    "    RANDOM_STATE = 41\n",
    "  \n",
    "    def get_classes(self):\n",
    "        return self.EMOTION_CLASSES\n",
    "  \n",
    "    def get_classes_dict(self):\n",
    "        return self.EMOTION_CLASSES_DICT\n",
    "\n",
    "    def _sequence_texts(self, texts):\n",
    "        tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        sequences = pad_sequences(tokenizer.texts_to_sequences(texts))\n",
    "        word_index = tokenizer.word_index\n",
    "        return sequences, word_index\n",
    "\n",
    "    def __init__(self, n_items=0):\n",
    "        data = pd.read_csv(self.FILENAME)\n",
    "\n",
    "        if n_items > 0:\n",
    "            data = data.iloc[0:n_items,:]\n",
    "\n",
    "        data[\"text\"] = data[\"SIT\"]\n",
    "        data[\"emotion\"] = data[\"Field1\"]\n",
    "\n",
    "        for emotion in self.get_classes():\n",
    "            data.loc[data[\"emotion\"] == emotion, \"emotion_int\"] = self.get_classes_dict()[emotion]\n",
    "\n",
    "        sequences, word_index = self._sequence_texts(data[\"text\"])\n",
    "        self.X = sequences\n",
    "        self.y = data[\"emotion_int\"].values\n",
    "        self.word_index = word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ISEARDataset()\n",
    "# dataset = ISEARDataset(200)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.X, dataset.y, test_size=0.3, random_state=dataset.RANDOM_STATE, stratify=dataset.y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=dataset.RANDOM_STATE, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (4292, 192)\n",
      "y_train.shape: (4292)\n",
      "X_valid.shape: (1074, 192)\n",
      "y_valid.shape: (1074)\n",
      "X_test.shape: (2300, 192)\n",
      "y_test.shape: (2300)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: (%d, %d)\" % X_train.shape)\n",
    "print(\"y_train.shape: (%d)\" % y_train.shape)\n",
    "\n",
    "print(\"X_valid.shape: (%d, %d)\" % X_valid.shape)\n",
    "print(\"y_valid.shape: (%d)\" % y_valid.shape)\n",
    "\n",
    "print(\"X_test.shape: (%d, %d)\" % X_test.shape)\n",
    "print(\"y_test.shape: (%d)\" % y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"pytorch_isear_X_train.npy\", X_train)\n",
    "np.save(\"pytorch_isear_X_valid.npy\", X_valid)\n",
    "np.save(\"pytorch_isear_X_test.npy\", X_test)\n",
    "\n",
    "np.save(\"pytorch_isear_y_train.npy\", y_train)\n",
    "np.save(\"pytorch_isear_y_valid.npy\", y_valid)\n",
    "np.save(\"pytorch_isear_y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"pytorch_isear_X_train.npy\")\n",
    "X_valid = np.load(\"pytorch_isear_X_valid.npy\")\n",
    "X_test = np.load(\"pytorch_isear_X_test.npy\")\n",
    "\n",
    "y_train = np.load(\"pytorch_isear_y_train.npy\")\n",
    "y_valid = np.load(\"pytorch_isear_y_valid.npy\")\n",
    "y_test = np.load(\"pytorch_isear_y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class dictionary: {'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n",
      "class labels: ['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']\n",
      "number of bins: 7\n"
     ]
    }
   ],
   "source": [
    "dic = dataset.get_classes_dict()\n",
    "labels = dataset.get_classes()\n",
    "n_classes = len(labels)\n",
    "print(\"class dictionary: %s\" % dic)\n",
    "print(\"class labels: %s\" % labels)\n",
    "print(\"number of bins: %s\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins: [0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = list(range(0, n_classes + 1))\n",
    "print(\"bins:\", bins)\n",
    "hist, _ = np.histogram(y_train, bins=bins)\n",
    "\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "plt.bar(y_pos, hist, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Number')\n",
    "plt.title('Emotions')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape: torch.Size([4292, 1, 1, 1])\n",
      "y_onehot.shape: torch.Size([4292, 7, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch import autograd\n",
    "\n",
    "def make_one_hot(labels, C=2):\n",
    "    '''\n",
    "    Converts an integer label torch.autograd.Variable to a one-hot Variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : torch.autograd.Variable of torch.cuda.LongTensor\n",
    "        N x 1 x H x W, where N is batch size. \n",
    "        Each value is an integer representing correct classification.\n",
    "    C : integer. \n",
    "        number of classes in labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target : torch.autograd.Variable of torch.cuda.FloatTensor\n",
    "        N x C x H x W, where C is class number. One-hot encoded.\n",
    "    '''\n",
    "    one_hot = torch.FloatTensor(labels.size(0), C, labels.size(2), labels.size(3)).zero_()\n",
    "    target = one_hot.scatter_(1, labels.data, 1)\n",
    "    \n",
    "    target = autograd.Variable(target)\n",
    "        \n",
    "    return target\n",
    "  \n",
    "y = torch.LongTensor(y_train).view(-1, 1, 1, 1)\n",
    "print(\"y.shape:\", y.shape)\n",
    "y_onehot = make_one_hot(y, C=7)\n",
    "print(\"y_onehot.shape:\", y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class ISEAR_Tensor_Dataset(data.TensorDataset):\n",
    "  \n",
    "  def __init__(self, text, emotion, num_class=2):\n",
    "    X = torch.LongTensor(text)\n",
    "    y = torch.LongTensor(emotion).view(-1, 1, 1, 1)\n",
    "    y_onehot = make_one_hot(y, num_class)\n",
    "    y_onehot = y_onehot.view(y_onehot.shape[0], y_onehot.shape[1])\n",
    "    tensors = []\n",
    "    tensors.append(X)\n",
    "    tensors.append(y_onehot)\n",
    "    super().__init__(*tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ISEAR_Tensor_Dataset(X_train, y_train, num_class=7)\n",
    "valid_dataset = ISEAR_Tensor_Dataset(X_valid, y_valid, num_class=7)\n",
    "test_dataset = ISEAR_Tensor_Dataset(X_test, y_test, num_class=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.tensors[0].shape: torch.Size([4292, 192])\n",
      "train_dataset.tensors[1].shape: torch.Size([4292, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset.tensors[0].shape:\", train_dataset.tensors[0].shape)\n",
    "print(\"train_dataset.tensors[1].shape:\", train_dataset.tensors[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length: 4292\n",
      "valid_dataset length: 1074\n",
      "test_dataset length: 2300\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset length:\", len(train_dataset))\n",
    "print(\"valid_dataset length:\", len(valid_dataset))\n",
    "print(\"test_dataset length:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../../Datasets/WordEmbeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "vocabulary_size = min(len(dataset.word_index)+1, NUM_WORDS)\n",
    "embedding_weights = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in dataset.word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_weights[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_weights[i] = np.random.normal(0,np.sqrt(0.25), EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_embedding_weights = \"pytorch_isear_embedded_matrix_word2vec_googlenews.npy\"\n",
    "np.save(file_embedding_weights, embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_weights.shape: (9014, 300)\n"
     ]
    }
   ],
   "source": [
    "file_embedding_weights = \"pytorch_isear_embedded_matrix_word2vec_googlenews.npy\"\n",
    "embedding_weights = np.load(file_embedding_weights)\n",
    "print(\"embedding_weights.shape:\", embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=EMBEDDING_DIM, _weight=torch.Tensor(embedding_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "for param in embedding.parameters():\n",
    "    print(\"embedding.requires_grad:\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, input_dim, embedding_dim=300, \n",
    "                 output_dim=7, filter_dim=100, filter_sizes=[3,4,5], drop_rate=0.5):\n",
    "        super(ConvNetClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.filter_dim = filter_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        self.conv2d_1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=self.filter_dim, \n",
    "            kernel_size=(self.filter_sizes[0], self.embedding_dim))\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=self.filter_dim, \n",
    "            kernel_size=(self.filter_sizes[1], self.embedding_dim))\n",
    "        \n",
    "        self.conv2d_3 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=self.filter_dim, \n",
    "            kernel_size=(self.filter_sizes[2], self.embedding_dim))\n",
    "        \n",
    "        self.maxpool2d_1 = nn.MaxPool2d(\n",
    "            kernel_size=(self.input_dim-self.filter_sizes[0]+1, 1), \n",
    "            stride=1)\n",
    "        \n",
    "        self.maxpool2d_2 = nn.MaxPool2d(\n",
    "            kernel_size=(self.input_dim-self.filter_sizes[1]+1, 1), \n",
    "            stride=1)\n",
    "        \n",
    "        self.maxpool2d_3 = nn.MaxPool2d(\n",
    "            kernel_size=(self.input_dim-self.filter_sizes[2]+1, 1), \n",
    "            stride=1)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=self.embedding_dim, out_features=self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        reshaped = embedded.view(embedded.shape[0], 1, embedded.shape[1], self.embedding_dim)\n",
    "        \n",
    "        conv2d_1 = F.relu(self.conv2d_1(reshaped))\n",
    "        conv2d_2 = F.relu(self.conv2d_2(reshaped))\n",
    "        conv2d_3 = F.relu(self.conv2d_3(reshaped))\n",
    "        \n",
    "        maxpool2d_1 = self.maxpool2d_1(conv2d_1)\n",
    "        maxpool2d_2 = self.maxpool2d_2(conv2d_2)\n",
    "        maxpool2d_3 = self.maxpool2d_3(conv2d_3)\n",
    "        \n",
    "        concat = torch.cat([maxpool2d_1, maxpool2d_2, maxpool2d_3], dim=1)\n",
    "        flatten = concat.view(concat.shape[0], concat.shape[1])\n",
    "        dropout = F.dropout(flatten, self.drop_rate)\n",
    "        output = F.softmax(self.linear(dropout))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "filter_dim = 100\n",
    "embedding_dim = 300\n",
    "filter_sizes = [3,4,5]\n",
    "drop_rate = 0.5\n",
    "output_dim = 7\n",
    "model = ConvNetClassifier(embedding, input_dim, embedding_dim, output_dim, filter_dim, filter_sizes, drop_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': <__main__.ISEAR_Tensor_Dataset at 0x7fc1d5fab710>,\n",
       " 'batch_size': 50,\n",
       " 'num_workers': 0,\n",
       " 'collate_fn': <function torch.utils.data.dataloader.default_collate(batch)>,\n",
       " 'pin_memory': False,\n",
       " 'drop_last': False,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7fc1d4d54d30>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7fc1d4d54d68>,\n",
       " '_DataLoader__initialized': True}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, device, optimiser, loss_fn=nn.BCELoss(), log_interval=100):\n",
    "    #####################\n",
    "    # Train model\n",
    "    #####################\n",
    "\n",
    "    # switch model to training mode, clear gradient accumulators\n",
    "    model.train()\n",
    "    # model.hidden = model.init_hidden()\n",
    "    \n",
    "    train_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    all_pred = []\n",
    "    all_target = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        train_loss += loss.data # sum up batch loss\n",
    "        \n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        pred = pred.view(pred.size(0))\n",
    "\n",
    "        target = target.max(1, keepdim=True)[1]\n",
    "        target = target.view(target.size(0))\n",
    "\n",
    "        correct = pred.eq(target.view_as(pred)).sum()\n",
    "        total_correct += correct\n",
    "\n",
    "        all_pred += pred.cpu().numpy().tolist()\n",
    "        all_target += target.cpu().numpy().tolist()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}\\t/\\t{}\\t({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "    \n",
    "    print('Train: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, total_correct, len(data_loader.dataset),\n",
    "        100. * total_correct / len(data_loader.dataset)))\n",
    "    \n",
    "    return train_loss, all_pred, all_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, loss_fn=nn.BCELoss()):\n",
    "    #####################\n",
    "    # Evaluation model\n",
    "    #####################\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    all_pred = []\n",
    "    all_target = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            eval_loss += loss_fn(output, target).data # sum up batch loss\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            pred = pred.view(pred.size(0))\n",
    "            \n",
    "            target = target.max(1, keepdim=True)[1]\n",
    "            target = target.view(target.size(0))\n",
    "            \n",
    "            correct = pred.eq(target.view_as(pred)).sum()\n",
    "            total_correct += correct\n",
    "\n",
    "            all_pred += pred.cpu().numpy().tolist()\n",
    "            all_target += target.cpu().numpy().tolist()\n",
    "\n",
    "    print('Evaluate: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        eval_loss, total_correct, len(data_loader.dataset),\n",
    "        100. * total_correct / len(data_loader.dataset)))\n",
    "    \n",
    "    return eval_loss, all_pred, all_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-27b238d65cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-b5a0f535f95b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, data_loader, device, optimiser, loss_fn, log_interval)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-99b979804807>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mreshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "log_interval = 10\n",
    "max_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_hist = np.zeros(max_epochs)\n",
    "eval_loss_hist = np.zeros(max_epochs)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "    train_loss, train_pred, train_target = train(epoch, model, train_loader, device, optimiser, loss_fn, log_interval)\n",
    "    train_loss_hist[epoch] = train_loss\n",
    "    \n",
    "    valid_loss, valid_pred, valid_target = evaluate(model, valid_loader, device, loss_fn=nn.BCELoss())\n",
    "    eval_loss_hist[epoch] = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_hist, color='skyblue')\n",
    "plt.plot(eval_loss_hist, color='red')\n",
    "plt.ylabel('Historic Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
